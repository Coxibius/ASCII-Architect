{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14156414,"sourceType":"datasetVersion","datasetId":9022982}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. INSTALAR LIBRERÃAS\n!pip install transformers datasets accelerate -Uq\n\nimport os\nimport gc\nimport torch\nimport shutil\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\n# ==========================================\n# 2. CONFIGURACIÃ“N Y RUTAS\n# ==========================================\n# Ajusta esto segÃºn dÃ³nde subiste tus archivos en Kaggle\nBASE_PATH = \"/kaggle/input/arrows-and-boxes\" \n\n# Mapeo de Expertos\nTRAIN_FILES = {\n    \"BOX\":   f\"{BASE_PATH}/dataset_boxes.jsonl\",\n    \"ARROW\": f\"{BASE_PATH}/dataset_arrows.jsonl\"\n}\n\n# VerificaciÃ³n\nfor k, v in TRAIN_FILES.items():\n    if os.path.exists(v):\n        print(f\"âœ… {k}: Encontrado ({os.path.getsize(v)/1024:.2f} KB)\")\n    else:\n        print(f\"âŒ {k}: NO ENCONTRADO en {v} (Revisa la ruta)\")\n\n# ==========================================\n# 3. DICCIONARIO MAESTRO V18 (ARCHITECT)\n# ==========================================\nMODEL_CHECKPOINT = \"gpt2\" # GPT-2 Small (124M)\n\nSPECIAL_TOKENS = {\n    \"pad_token\": \"[PAD]\",\n    \"additional_special_tokens\": [\n        # Control\n        \"[START]\", \"[STOP]\", \"[VOID]\", \n        # Estructura HÃ­brida\n        \"â–‘\", \n        \"<L01>\", \"<L02>\", \"<L03>\", \"<L04>\", \"<L05>\", \"<L06>\", \"<L07>\", \"<L08>\", \"<L09>\", \"<L10>\",\n        \"[S:00]\", # En entrenamiento siempre es 0, pero lo mantenemos por compatibilidad\n        # Anchors y Primitivas de Caja\n        \"N\", \"S\", \"E\", \"W\", \n        \"+\", \"-\", \"|\",\n        # Flechas\n        \"^\", \"v\", \"<\", \">\",\n        # Tags del Prompt\n        \"[TYPE:BOX]\", \"[TYPE:ARROW]\",\n        \"[STYLE:SOLID]\",\n        \"[DIR:UP]\", \"[DIR:DOWN]\", \"[DIR:LEFT]\", \"[DIR:RIGHT]\",\n        # Dimensiones (Importante: El modelo tokenizarÃ¡ los nÃºmeros, pero enseÃ±amos el formato)\n        \"[DIM:\", \"[LEN:\" \n    ]\n}\n\n# ==========================================\n# 4. MOTOR DE ENTRENAMIENTO (The Gym)\n# ==========================================\ndef train_architect(expert_name, jsonl_path):\n    print(f\"\\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\")\n    print(f\"ğŸ—ï¸  CONSTRUYENDO EXPERTO: {expert_name}\")\n    print(f\"â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\")\n    \n    output_dir = f\"./expert_{expert_name.lower()}\"\n    \n    # Tokenizer\n    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n    tokenizer.add_special_tokens(SPECIAL_TOKENS)\n    \n    # Dataset\n    dataset = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n    \n    def format_fn(x):\n        text = f\"{x['prompt']} {x['completion']}\"\n        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=128) # Subimos a 128 por si las cajas son grandes\n    \n    tokenized = dataset.map(format_fn, batched=False, remove_columns=dataset.column_names)\n    \n    # Modelo\n    model = GPT2LMHeadModel.from_pretrained(MODEL_CHECKPOINT)\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # Argumentos (Overfitting tÃ¡ctico para precisiÃ³n geomÃ©trica)\n    args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=10,          # 10 Vueltas es un buen punto de partida\n        per_device_train_batch_size=32, \n        learning_rate=5e-4,\n        save_strategy=\"no\",\n        logging_steps=50,\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(\n        model=model, \n        args=args, \n        train_dataset=tokenized, \n        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n    )\n    \n    trainer.train()\n    \n    # Guardar\n    print(f\"ğŸ’¾ Guardando planos en {output_dir}...\")\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # Limpiar\n    del model, trainer, tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# --- EJECUCIÃ“N ---\n# Solo entrenamos si encontrÃ³ los archivos\nif os.path.exists(TRAIN_FILES[\"BOX\"]):\n    train_architect(\"BOX\", TRAIN_FILES[\"BOX\"])\n\nif os.path.exists(TRAIN_FILES[\"ARROW\"]):\n    train_architect(\"ARROW\", TRAIN_FILES[\"ARROW\"])\n\n# ==========================================\n# 5. EMPAQUETADO (Delivery)\n# ==========================================\nprint(\"\\nğŸ“¦ EMPAQUETANDO HERRAMIENTAS...\")\nfrom IPython.display import FileLink\n\nzip_name = \"ASCII_Architect_V1_Models\"\nos.makedirs(\"Architect_Package\", exist_ok=True)\n\nfor folder in [\"expert_box\", \"expert_arrow\"]:\n    if os.path.exists(folder):\n        shutil.move(folder, f\"Architect_Package/{folder}\")\n\nshutil.make_archive(zip_name, 'zip', \"Architect_Package\")\nprint(f\"âœ… Â¡LISTO! Descarga tus arquitectos:\")\nFileLink(f\"{zip_name}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:36:17.668482Z","iopub.execute_input":"2025-12-14T18:36:17.668672Z","iopub.status.idle":"2025-12-14T18:38:21.916931Z","shell.execute_reply.started":"2025-12-14T18:36:17.668654Z","shell.execute_reply":"2025-12-14T18:38:21.916310Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"2025-12-14 18:38:00.437505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765737480.648310      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765737480.708906      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"âŒ BOX: NO ENCONTRADO en /kaggle/input/dataset-v18-architect/dataset_boxes.jsonl (Revisa la ruta)\nâŒ ARROW: NO ENCONTRADO en /kaggle/input/dataset-v18-architect/dataset_arrows.jsonl (Revisa la ruta)\n\nğŸ“¦ EMPAQUETANDO HERRAMIENTAS...\nâœ… Â¡LISTO! Descarga tus arquitectos:\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ASCII_Architect_V1_Models.zip","text/html":"<a href='ASCII_Architect_V1_Models.zip' target='_blank'>ASCII_Architect_V1_Models.zip</a><br>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# 1. INSTALAR LIBRERÃAS (Incluyendo el parche de protobuf)\n!pip install transformers datasets accelerate protobuf -Uq\n\nimport os\nimport time\n\nprint(\"âœ… LibrerÃ­as instaladas.\")\nprint(\"ğŸ”„ REINICIANDO KERNEL AUTOMÃTICAMENTE PARA APLICAR CAMBIOS...\")\n# Esto mata la sesiÃ³n actual y arranca una limpia con las librerÃ­as nuevas\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:40:53.429797Z","iopub.execute_input":"2025-12-14T18:40:53.430513Z","execution_failed":"2025-12-14T18:40:59.597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# CONFIGURACIÃ“N DE RUTAS\n# Basado en tu captura, tu dataset se llama \"arrows-and-boxes\"\nBASE_PATH = \"/kaggle/input/arrows-and-boxes\"\n\nFILES = {\n    \"BOX\":   f\"{BASE_PATH}/dataset_boxes.jsonl\",\n    \"ARROW\": f\"{BASE_PATH}/dataset_arrows.jsonl\"\n}\n\nprint(\"ğŸ•µï¸ INSPECCIONANDO RUTAS...\")\n\nall_good = True\nfor name, path in FILES.items():\n    if os.path.exists(path):\n        size = os.path.getsize(path) / 1024\n        print(f\"   âœ… {name}: ENCONTRADO ({size:.2f} KB) -> {path}\")\n    else:\n        print(f\"   âŒ {name}: NO EXISTE en {path}\")\n        all_good = False\n\nif not all_good:\n    print(\"\\nâš ï¸ ALERTA: Revisa el nombre de la carpeta en el panel derecho (Input).\")\n    print(\"Contenido real de /kaggle/input:\")\n    try:\n        print(os.listdir(\"/kaggle/input\"))\n        print(f\"Contenido de {BASE_PATH}:\")\n        print(os.listdir(BASE_PATH))\n    except:\n        print(\"No pude leer los directorios.\")\nelse:\n    print(\"\\nğŸš€ TODO LISTO. PUEDES PASAR A ENTRENAR.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:41:04.874325Z","iopub.execute_input":"2025-12-14T18:41:04.874592Z","iopub.status.idle":"2025-12-14T18:41:04.901459Z","shell.execute_reply.started":"2025-12-14T18:41:04.874572Z","shell.execute_reply":"2025-12-14T18:41:04.900732Z"}},"outputs":[{"name":"stdout","text":"ğŸ•µï¸ INSPECCIONANDO RUTAS...\n   âœ… BOX: ENCONTRADO (1708.92 KB) -> /kaggle/input/arrows-and-boxes/dataset_boxes.jsonl\n   âœ… ARROW: ENCONTRADO (644.83 KB) -> /kaggle/input/arrows-and-boxes/dataset_arrows.jsonl\n\nğŸš€ TODO LISTO. PUEDES PASAR A ENTRENAR.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gc\nimport torch\nimport shutil\nfrom datasets import load_dataset\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\n# 1. DICCIONARIO MAESTRO V18 (Architect)\nMODEL_CHECKPOINT = \"gpt2\"\n\nSPECIAL_TOKENS = {\n    \"pad_token\": \"[PAD]\",\n    \"additional_special_tokens\": [\n        \"[START]\", \"[STOP]\", \"[VOID]\", \n        \"â–‘\", \n        \"<L01>\", \"<L02>\", \"<L03>\", \"<L04>\", \"<L05>\", \"<L06>\", \"<L07>\", \"<L08>\", \"<L09>\", \"<L10>\",\n        \"[S:00]\", \n        \"N\", \"S\", \"E\", \"W\", \"+\", \"-\", \"|\", \"^\", \"v\", \"<\", \">\",\n        \"[TYPE:BOX]\", \"[TYPE:ARROW]\", \"[STYLE:SOLID]\",\n        \"[DIR:UP]\", \"[DIR:DOWN]\", \"[DIR:LEFT]\", \"[DIR:RIGHT]\", \"[DIM:\", \"[LEN:\" \n    ]\n}\n\n# 2. FUNCIÃ“N DE ENTRENAMIENTO\ndef train_architect(expert_name, jsonl_path):\n    print(f\"\\nğŸ—ï¸  CONSTRUYENDO EXPERTO: {expert_name}\")\n    \n    output_dir = f\"./expert_{expert_name.lower()}\"\n    \n    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n    tokenizer.add_special_tokens(SPECIAL_TOKENS)\n    \n    dataset = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n    \n    def format_fn(x):\n        text = f\"{x['prompt']} {x['completion']}\"\n        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=128)\n    \n    tokenized = dataset.map(format_fn, batched=False, remove_columns=dataset.column_names)\n    \n    model = GPT2LMHeadModel.from_pretrained(MODEL_CHECKPOINT)\n    model.resize_token_embeddings(len(tokenizer))\n    \n    args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=10,\n        per_device_train_batch_size=32,\n        learning_rate=5e-4,\n        save_strategy=\"no\",\n        logging_steps=50,\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(model=model, args=args, train_dataset=tokenized, data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n    trainer.train()\n    \n    print(f\"ğŸ’¾ Guardando {expert_name}...\")\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    del model, trainer, tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# 3. EJECUTAR (Usando las rutas validadas en Celda 2)\n# Nota: FILES viene de la celda anterior\ntrain_architect(\"BOX\", FILES[\"BOX\"])\ntrain_architect(\"ARROW\", FILES[\"ARROW\"])\n\nprint(\"\\nâœ¨ ENTRENAMIENTO COMPLETADO.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T18:41:16.436727Z","iopub.execute_input":"2025-12-14T18:41:16.436995Z","iopub.status.idle":"2025-12-14T19:20:19.401732Z","shell.execute_reply.started":"2025-12-14T18:41:16.436975Z","shell.execute_reply":"2025-12-14T19:20:19.401007Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 18:41:22.182755: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765737682.204504     160 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765737682.211060     160 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"\nğŸ—ï¸  CONSTRUYENDO EXPERTO: BOX\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10f96acdf8a941b28b8ee9a80384b5e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e17b3994b50a449abeeb782b0ad88304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9bb80c7f6cf4aaea783db2890c9b070"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aa8b77a8074c2480a20bf616c0199c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f4f43917f34d50b5d08cd4f78c1cf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba6c86b728224c3b80634e6503dbaf19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895d788a857244c7b5539769fb911406"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bf16da548b34a5e87f20c4522dd3d42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28590a2580c84fc1b25af351d4e59132"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [790/790 19:48, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.080200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.271700</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.159000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.137300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.105900</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.075300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.066000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.069500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.059800</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.050300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.048700</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.048000</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.047200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.046700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.046400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ’¾ Guardando BOX...\n\nğŸ—ï¸  CONSTRUYENDO EXPERTO: ARROW\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f08b434be08644f29281edd5a931ba75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5698e863e0e402b9a768c3affc226f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [790/790 18:40, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.817300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.345600</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.214600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.180500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.165300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.140300</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.117600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.108500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.107200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.107200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.104300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.104500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.102000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.102800</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.103100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ’¾ Guardando ARROW...\n\nâœ¨ ENTRENAMIENTO COMPLETADO.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from IPython.display import FileLink\nimport os\nimport shutil\n\nzip_name = \"ASCII_Architect_V1_Models\"\nos.makedirs(\"Architect_Package\", exist_ok=True)\n\nfor folder in [\"expert_box\", \"expert_arrow\"]:\n    if os.path.exists(folder):\n        shutil.move(folder, f\"Architect_Package/{folder}\")\n\nshutil.make_archive(zip_name, 'zip', \"Architect_Package\")\nprint(f\"âœ… DESCARGA LISTA:\")\nFileLink(f\"{zip_name}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T19:21:36.787127Z","iopub.execute_input":"2025-12-14T19:21:36.788208Z","iopub.status.idle":"2025-12-14T19:22:25.350899Z","shell.execute_reply.started":"2025-12-14T19:21:36.788179Z","shell.execute_reply":"2025-12-14T19:22:25.350025Z"}},"outputs":[{"name":"stdout","text":"âœ… DESCARGA LISTA:\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ASCII_Architect_V1_Models.zip","text/html":"<a href='ASCII_Architect_V1_Models.zip' target='_blank'>ASCII_Architect_V1_Models.zip</a><br>"},"metadata":{}}],"execution_count":3}]}
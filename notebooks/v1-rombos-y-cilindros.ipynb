{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14157559,"sourceType":"datasetVersion","datasetId":9023805}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1. INSTALAR LIBRERÃAS\n!pip install transformers datasets accelerate protobuf -Uq\n\nimport os\nimport time\n\nprint(\"âœ… LibrerÃ­as instaladas.\")\nprint(\"ğŸ”„ REINICIANDO KERNEL PARA APLICAR CAMBIOS...\")\nos.kill(os.getpid(), 9)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:46:25.269660Z","iopub.execute_input":"2025-12-14T21:46:25.269915Z","execution_failed":"2025-12-14T21:47:58.150Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.2 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\n\n# CONFIGURACIÃ“N DE RUTAS\n# Ajusta el nombre si Kaggle le puso otro, pero segÃºn tu captura es este:\nBASE_PATH = \"/kaggle/input/ascii-architect-v1-2\"\n\nFILES = {\n    \"DIAMOND\":  f\"{BASE_PATH}/dataset_diamonds.jsonl\",\n    \"CYLINDER\": f\"{BASE_PATH}/dataset_cylinders.jsonl\"\n}\n\nprint(\"ğŸ•µï¸ INSPECCIONANDO RUTAS...\")\nall_good = True\nfor name, path in FILES.items():\n    if os.path.exists(path):\n        print(f\"   âœ… {name}: ENCONTRADO -> {path}\")\n    else:\n        print(f\"   âŒ {name}: NO EXISTE en {path}\")\n        all_good = False\n\nif all_good:\n    print(\"\\nğŸš€ DATOS LOCALIZADOS. LISTO PARA ENTRENAR.\")\nelse:\n    print(\"\\nâš ï¸ Revisa el panel derecho para ver el nombre real de la carpeta.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:48:13.132249Z","iopub.execute_input":"2025-12-14T21:48:13.132764Z","iopub.status.idle":"2025-12-14T21:48:13.143205Z","shell.execute_reply.started":"2025-12-14T21:48:13.132740Z","shell.execute_reply":"2025-12-14T21:48:13.142485Z"}},"outputs":[{"name":"stdout","text":"ğŸ•µï¸ INSPECCIONANDO RUTAS...\n   âœ… DIAMOND: ENCONTRADO -> /kaggle/input/ascii-architect-v1-2/dataset_diamonds.jsonl\n   âœ… CYLINDER: ENCONTRADO -> /kaggle/input/ascii-architect-v1-2/dataset_cylinders.jsonl\n\nğŸš€ DATOS LOCALIZADOS. LISTO PARA ENTRENAR.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer\n\nMODEL_CHECKPOINT = \"gpt2\"\n\n# DICCIONARIO MAESTRO V18 (EXPANDIDO)\nSPECIAL_TOKENS = {\n    \"pad_token\": \"[PAD]\",\n    \"additional_special_tokens\": [\n        \"[START]\", \"[STOP]\", \"[VOID]\", \n        \"â–‘\", \n        \"<L01>\", \"<L02>\", \"<L03>\", \"<L04>\", \"<L05>\", \"<L06>\", \"<L07>\", \"<L08>\", \"<L09>\", \"<L10>\", \"<L11>\", \"<L12>\", \"<L13>\", \"<L14>\", \"<L15>\",\n        \"[S:00]\", \n        \"N\", \"S\", \"E\", \"W\", \"+\", \"-\", \"|\", \"^\", \"v\", \"<\", \">\", \"/\", \"\\\\\", \".\", \"'\",\n        # TIPOS NUEVOS\n        \"[TYPE:BOX]\", \"[TYPE:ARROW]\", \"[TYPE:DIAMOND]\", \"[TYPE:CYLINDER]\",\n        \"[STYLE:SOLID]\", \"[STYLE:DB]\",\n        \"[DIR:UP]\", \"[DIR:DOWN]\", \"[DIR:LEFT]\", \"[DIR:RIGHT]\", \n        \"[DIM:\", \"[LEN:\", \"[SIZE:\"\n    ]\n}\n\nprint(\"âš™ï¸ ConfiguraciÃ³n V18 cargada con soporte para Rombos y Cilindros.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:48:16.241680Z","iopub.execute_input":"2025-12-14T21:48:16.242220Z","iopub.status.idle":"2025-12-14T21:48:20.555242Z","shell.execute_reply.started":"2025-12-14T21:48:16.242197Z","shell.execute_reply":"2025-12-14T21:48:20.554559Z"}},"outputs":[{"name":"stdout","text":"âš™ï¸ ConfiguraciÃ³n V18 cargada con soporte para Rombos y Cilindros.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import gc\nfrom datasets import load_dataset\nfrom transformers import GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n\ndef train_expert(expert_name, jsonl_path):\n    print(f\"\\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\")\n    print(f\"ğŸ’ CONSTRUYENDO EXPERTO: {expert_name}\")\n    print(f\"â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\")\n    \n    output_dir = f\"./expert_{expert_name.lower()}\"\n    \n    # 1. Tokenizer\n    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n    tokenizer.add_special_tokens(SPECIAL_TOKENS)\n    \n    # 2. Dataset\n    dataset = load_dataset(\"json\", data_files=jsonl_path, split=\"train\")\n    \n    def format_fn(x):\n        text = f\"{x['prompt']} {x['completion']}\"\n        return tokenizer(text, truncation=True, padding=\"max_length\", max_length=150)\n    \n    tokenized = dataset.map(format_fn, batched=False, remove_columns=dataset.column_names)\n    \n    # 3. Modelo\n    model = GPT2LMHeadModel.from_pretrained(MODEL_CHECKPOINT)\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # 4. Argumentos (Overfitting para precisiÃ³n)\n    args = TrainingArguments(\n        output_dir=output_dir,\n        overwrite_output_dir=True,\n        num_train_epochs=10,\n        per_device_train_batch_size=32,\n        learning_rate=5e-4,\n        save_strategy=\"no\",\n        logging_steps=50,\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(model=model, args=args, train_dataset=tokenized, data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False))\n    trainer.train()\n    \n    # 5. Guardar\n    print(f\"ğŸ’¾ Guardando {expert_name}...\")\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    \n    # 6. Limpiar\n    del model, trainer, tokenizer\n    torch.cuda.empty_cache()\n    gc.collect()\n\n# --- EJECUTAR ---\nif os.path.exists(FILES[\"DIAMOND\"]):\n    train_expert(\"DIAMOND\", FILES[\"DIAMOND\"])\n\nif os.path.exists(FILES[\"CYLINDER\"]):\n    train_expert(\"CYLINDER\", FILES[\"CYLINDER\"])\n\nprint(\"\\nâœ¨ ENTRENAMIENTO DE NUEVOS EXPERTOS COMPLETADO.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T21:48:50.087535Z","iopub.execute_input":"2025-12-14T21:48:50.087806Z","iopub.status.idle":"2025-12-14T22:36:33.335683Z","shell.execute_reply.started":"2025-12-14T21:48:50.087786Z","shell.execute_reply":"2025-12-14T22:36:33.334896Z"}},"outputs":[{"name":"stderr","text":"2025-12-14 21:48:56.951571: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1765748937.139241     109 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1765748937.195833     109 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\nğŸ’ CONSTRUYENDO EXPERTO: DIAMOND\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b291446c5a8344fe96dd299213604aad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fc06ebf3138435883ef6c25b8ed29dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ed067cc726c4a2d82c5db2c5f36b152"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"204c9a0a2c4c4a44883300b68d72fe30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bef9559591394581972eefb928be96cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f7867c80ae4eaaa8fa845ef569440a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b770f3a798d42bb8da408d15c8ae033"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fba12ef5b1ac41efb972fea4554f9e88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14cb422fe7c5401fb25d160097f95496"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [790/790 23:43, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>2.546300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.279100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.054400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.023700</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.016400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.015500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.015700</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.015400</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.015500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ’¾ Guardando DIAMOND...\n\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\nğŸ’ CONSTRUYENDO EXPERTO: CYLINDER\nâ–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬â–¬\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e882aac825e44f6b5c4332e2aee78d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7879b410c3c8415e95eb74a3ce4522b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='790' max='790' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [790/790 23:07, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.858500</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.218500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.108300</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.079500</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.056400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.051000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.048500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.047700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.046600</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.046800</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.046000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.046300</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.045800</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.045900</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.045900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ğŸ’¾ Guardando CYLINDER...\n\nâœ¨ ENTRENAMIENTO DE NUEVOS EXPERTOS COMPLETADO.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import pipeline\n\ndef test_model(name, prompt):\n    path = f\"./expert_{name.lower()}\"\n    print(f\"\\nğŸ§ª PROBANDO {name}...\")\n    try:\n        gen = pipeline(\"text-generation\", model=path, tokenizer=path, device=0)\n        out = gen(prompt, max_length=150, pad_token_id=50256, do_sample=False)\n        print(out[0]['generated_text'].replace(\"â–‘\", \" \").replace(prompt, \"\"))\n        del gen\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n# Test Rombo (TamaÃ±o 4)\ntest_mod","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:36:41.904635Z","iopub.execute_input":"2025-12-14T22:36:41.905833Z","iopub.status.idle":"2025-12-14T22:36:42.118314Z","shell.execute_reply.started":"2025-12-14T22:36:41.905802Z","shell.execute_reply":"2025-12-14T22:36:42.117058Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_109/1090406591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Test Rombo (TamaÃ±o 4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mtest_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test_mod' is not defined"],"ename":"NameError","evalue":"name 'test_mod' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\nzip_name = \"ASCII_Architect_V2_Expansion\"\nos.makedirs(\"Expansion_Pack\", exist_ok=True)\n\n# Mover carpetas\nfor folder in [\"expert_diamond\", \"expert_cylinder\"]:\n    if os.path.exists(folder):\n        shutil.move(folder, f\"Expansion_Pack/{folder}\")\n\n# Zipear\nshutil.make_archive(zip_name, 'zip', \"Expansion_Pack\")\n\nprint(f\"âœ… PAQUETE DE EXPANSIÃ“N LISTO:\")\nFileLink(f\"{zip_name}.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:37:44.106745Z","iopub.execute_input":"2025-12-14T22:37:44.107567Z","iopub.status.idle":"2025-12-14T22:38:32.418372Z","shell.execute_reply.started":"2025-12-14T22:37:44.107539Z","shell.execute_reply":"2025-12-14T22:38:32.417709Z"}},"outputs":[{"name":"stdout","text":"âœ… PAQUETE DE EXPANSIÃ“N LISTO:\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ASCII_Architect_V2_Expansion.zip","text/html":"<a href='ASCII_Architect_V2_Expansion.zip' target='_blank'>ASCII_Architect_V2_Expansion.zip</a><br>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\nimport re\nimport os\n\ndef test_architect_expert(name, prompt):\n    # CORRECCIÃ“N: Buscamos dentro de Expansion_Pack porque ya los moviste ahÃ­\n    folder_name = f\"expert_{name.lower()}\"\n    path_root = f\"./{folder_name}\"\n    path_pack = f\"./Expansion_Pack/{folder_name}\"\n    \n    # Detectar dÃ³nde demonios estÃ¡n\n    if os.path.exists(path_root):\n        path = path_root\n    elif os.path.exists(path_pack):\n        path = path_pack\n    else:\n        print(f\"âŒ NO ENCUENTRO EL MODELO '{folder_name}'. Â¿Se borrÃ³?\")\n        return\n\n    print(f\"\\n========================================\")\n    print(f\"ğŸ’ PROBANDO EXPERTO: {name}\")\n    print(f\"ğŸ“‚ Desde: {path}\")\n    print(f\"========================================\")\n    \n    try:\n        # Cargar Pipeline\n        gen = pipeline(\"text-generation\", model=path, tokenizer=path, device=0)\n        \n        # Generar\n        out = gen(prompt, max_length=200, pad_token_id=50256, do_sample=False)\n        \n        # Limpieza\n        raw_text = out[0]['generated_text']\n        content = raw_text.replace(prompt, \"\").replace(\"[STOP]\", \"\")\n        content = content.replace(\"â–‘\", \" \")\n        content = re.sub(r'<L\\d+>', '', content)\n        content = re.sub(r'\\[S:\\d+\\]', '', content)\n        \n        print(content)\n        \n        del gen\n        torch.cuda.empty_cache()\n        \n    except Exception as e:\n        print(f\"âŒ Error: {e}\")\n\n# --- BATERÃA DE PRUEBAS ---\ntest_architect_expert(\"DIAMOND\", \"[TYPE:DIAMOND] [STYLE:SOLID] [SIZE:4]\")\ntest_architect_expert(\"CYLINDER\", \"[TYPE:CYLINDER] [STYLE:DB] [DIM:10x5]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T22:42:44.266000Z","iopub.execute_input":"2025-12-14T22:42:44.266776Z","iopub.status.idle":"2025-12-14T22:42:49.834411Z","shell.execute_reply.started":"2025-12-14T22:42:44.266750Z","shell.execute_reply":"2025-12-14T22:42:49.833669Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\n========================================\nğŸ’ PROBANDO EXPERTO: DIAMOND\nğŸ“‚ Desde: ./Expansion_Pack/expert_diamond\n========================================\n","output_type":"stream"},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nBoth `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"   \n  \n  \n  \n    \n  \n     \n  \n     \n  \n  \n  \n\n========================================\nğŸ’ PROBANDO EXPERTO: CYLINDER\nğŸ“‚ Desde: ./Expansion_Pack/expert_cylinder\n========================================\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nBoth `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n","output_type":"stream"},{"name":"stdout","text":"   \n  \n  \n  \n                                                                                         \n","output_type":"stream"}],"execution_count":9}]}
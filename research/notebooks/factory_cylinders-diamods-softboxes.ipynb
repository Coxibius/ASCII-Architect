{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14165235,"sourceType":"datasetVersion","datasetId":9029042}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# ğŸ“¦ INSTALACIÃ“N DE DEPENDENCIAS\n# Silenciamos la salida (-q) para mantener el notebook limpio\n!pip install -q transformers datasets accelerate\nprint(\"âœ… LibrerÃ­as instaladas correctamente.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:10:04.838037Z","iopub.execute_input":"2025-12-15T14:10:04.838188Z","iopub.status.idle":"2025-12-15T14:11:44.911983Z","shell.execute_reply.started":"2025-12-15T14:10:04.838174Z","shell.execute_reply":"2025-12-15T14:11:44.911014Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… LibrerÃ­as instaladas correctamente.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset\n\n# ğŸ§¹ FunciÃ³n para limpiar basura de la GPU\ndef clear_gpu_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"ğŸ§¹ Memoria GPU liberada.\")\n\n# âš™ï¸ ConfiguraciÃ³n Global\nBASE_MODEL = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(BASE_MODEL)\ntokenizer.pad_token = tokenizer.eos_token \n\nprint(\"âœ… Entorno configurado.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:13:12.996150Z","iopub.execute_input":"2025-12-15T14:13:12.996844Z","iopub.status.idle":"2025-12-15T14:13:13.006299Z","shell.execute_reply.started":"2025-12-15T14:13:12.996814Z","shell.execute_reply":"2025-12-15T14:13:13.005186Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ğŸ“ RUTAS DE LOS DATASETS (Ajusta esto segÃºn la pestaÃ±a \"Data\" de tu notebook)\n# Kaggle suele montar los datasets en ../input/\nBASE_INPUT_DIR = \"../input/cylinders-diamonds-softboxes\"\n\nPATH_CYLINDERS = os.path.join(BASE_INPUT_DIR, \"dataset_cylinders.jsonl\")\nPATH_DIAMONDS  = os.path.join(BASE_INPUT_DIR, \"dataset_diamonds.jsonl\")\nPATH_SOFTBOXES = os.path.join(BASE_INPUT_DIR, \"dataset_soft_boxes.jsonl\")\n\n# ğŸ EJECUTAR ENTRENAMIENTOS SECUENCIALES\n\n# 1. Entrenar Expert Cylinder (Base de Datos / Torre)\ntrain_expert_model(PATH_CYLINDERS, \"expert_cylinder\", num_epochs=3)\n\n# 2. Entrenar Expert Diamond (Decisiones)\ntrain_expert_model(PATH_DIAMONDS, \"expert_diamond\", num_epochs=3)\n\n# 3. Entrenar Expert SoftBox (Entidades Redondeadas)\ntrain_expert_model(PATH_SOFTBOXES, \"expert_softbox\", num_epochs=3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T14:14:31.841617Z","iopub.execute_input":"2025-12-15T14:14:31.842478Z","iopub.status.idle":"2025-12-15T15:26:36.440057Z","shell.execute_reply.started":"2025-12-15T14:14:31.842450Z","shell.execute_reply":"2025-12-15T15:26:36.439412Z"}},"outputs":[{"name":"stdout","text":"\nğŸš€ INICIANDO ENTRENAMIENTO PARA: expert_cylinder\nğŸ“‚ Dataset: ../input/cylinders-diamonds-softboxes/dataset_cylinders.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc93cac7ca1c49f5adf667e1320aaa39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12e902ad10f04305906de05c67b138d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e256ff3fbd5447b839eba41aacd058a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fe52bcafc634423820250307b791bca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f546a4fe154e1a8dafd085cf38ca68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cc7a65ae82949139e8eb9ac503b76ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64897263d6ef4abdb3e9407ae84cc347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a0e69a0c58b499b92fbbc4cc15f683e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58788471a42648bab328ec31a777d169"}},"metadata":{}},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 23:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.142600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.052400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.039000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.028700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.023900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.019400</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.017000</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.015600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.014900</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.014800</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.014700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… MODELO GUARDADO EN: ./expert_cylinder\n\nğŸš€ INICIANDO ENTRENAMIENTO PARA: expert_diamond\nğŸ“‚ Dataset: ../input/cylinders-diamonds-softboxes/dataset_diamonds.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd90dd5fe0d641829e66bae181a9a6dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f093452cc284255be1bdb6ff32a4e79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 24:20, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.129200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.024800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.015100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.010300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.007300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.006000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.004400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… MODELO GUARDADO EN: ./expert_diamond\n\nğŸš€ INICIANDO ENTRENAMIENTO PARA: expert_softbox\nğŸ“‚ Dataset: ../input/cylinders-diamonds-softboxes/dataset_soft_boxes.jsonl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8bb418cc7054f059501e665731bbc8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2d861e24ebc446ba31a63650d126b4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1125' max='1125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1125/1125 23:32, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.148600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.059500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.046600</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.033700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.027800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.024200</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.023100</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.021600</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.020800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.020200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.020000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… MODELO GUARDADO EN: ./expert_softbox\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'./expert_softbox'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def prepare_dataset(file_path):\n    \"\"\"Carga y tokeniza el dataset sin entrenar nada aun.\"\"\"\n    print(f\"ğŸ“‚ Cargando datos desde: {file_path}\")\n    \n    dataset = load_dataset('json', data_files=file_path, split='train')\n    \n    def tokenize_function(examples):\n        inputs = [p + c + tokenizer.eos_token for p, c in zip(examples[\"prompt\"], examples[\"completion\"])]\n        return tokenizer(inputs, truncation=True, max_length=512, padding=\"max_length\")\n\n    tokenized = dataset.map(tokenize_function, batched=True, remove_columns=[\"prompt\", \"completion\"])\n    print(f\"ğŸ“Š Muestras procesadas: {len(tokenized)}\")\n    return tokenized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T15:35:05.162352Z","iopub.execute_input":"2025-12-15T15:35:05.162983Z","iopub.status.idle":"2025-12-15T15:35:05.167892Z","shell.execute_reply.started":"2025-12-15T15:35:05.162955Z","shell.execute_reply":"2025-12-15T15:35:05.167146Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def run_training(tokenized_data, output_name):\n    \"\"\"Entrena el modelo con los datos ya procesados.\"\"\"\n    print(f\"\\nğŸš€ Iniciando entrenamiento para: {output_name}\")\n    \n    model = GPT2LMHeadModel.from_pretrained(BASE_MODEL)\n    model.resize_token_embeddings(len(tokenizer))\n    \n    training_args = TrainingArguments(\n        output_dir=f\"./results/{output_name}\",\n        overwrite_output_dir=True,\n        num_train_epochs=3,           # 3 Pasadas\n        per_device_train_batch_size=8,# Bajar a 4 si falla la memoria\n        save_steps=1000,\n        logging_steps=100,\n        learning_rate=5e-5,\n        fp16=True,                    # AceleraciÃ³n GPU\n        report_to=\"none\"\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_data,\n        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n    )\n    \n    trainer.train()\n    \n    # Guardar\n    final_path = f\"./{output_name}\"\n    model.save_pretrained(final_path)\n    tokenizer.save_pretrained(final_path)\n    print(f\"âœ… Guardado en: {final_path}\")\n    \n    # IMPORTANTE: Matar el modelo de la memoria\n    del model\n    del trainer\n    clear_gpu_memory() # Limpiar para el siguiente turno","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T15:36:19.605304Z","iopub.execute_input":"2025-12-15T15:36:19.605590Z","iopub.status.idle":"2025-12-15T15:36:19.611382Z","shell.execute_reply.started":"2025-12-15T15:36:19.605569Z","shell.execute_reply":"2025-12-15T15:36:19.610820Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# ğŸ“ Definir Rutas (Verifica que coincidan con tu input de Kaggle)\nBASE_INPUT = \"../input/cylinders-diamonds-softboxes\"\n\n# 1. CYLINDERS (Database Towers)\ntry:\n    data_cyl = prepare_dataset(os.path.join(BASE_INPUT, \"dataset_cylinders.jsonl\"))\n    run_training(data_cyl, \"expert_cylinder\")\nexcept Exception as e:\n    print(f\"âŒ Error en Cylinders: {e}\")\n\n# 2. DIAMONDS (Decisiones)\ntry:\n    data_dia = prepare_dataset(os.path.join(BASE_INPUT, \"dataset_diamonds.jsonl\"))\n    run_training(data_dia, \"expert_diamond\")\nexcept Exception as e:\n    print(f\"âŒ Error en Diamonds: {e}\")\n\n# 3. SOFTBOXES (Nodos)\ntry:\n    data_soft = prepare_dataset(os.path.join(BASE_INPUT, \"dataset_soft_boxes.jsonl\"))\n    run_training(data_soft, \"expert_softbox\")\nexcept Exception as e:\n    print(f\"âŒ Error en SoftBoxes: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport zipfile\n\ndef compress_models():\n    # Nombre del archivo final\n    zip_name = \"ASCII_Architect_V2_Models.zip\"\n    \n    # Las carpetas que REALMENTE queremos (ignorando \"results\" que pesa mucho)\n    folders_to_save = [\n        \"expert_cylinder\", \n        \"expert_diamond\", \n        \"expert_softbox\"\n    ]\n    \n    print(f\"ğŸ“¦ Iniciando compresiÃ³n de: {folders_to_save}...\")\n    \n    with zipfile.ZipFile(zip_name, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        found_any = False\n        for folder in folders_to_save:\n            # Ruta completa en Kaggle\n            folder_path = os.path.join(\"/kaggle/working\", folder)\n            \n            if os.path.exists(folder_path):\n                found_any = True\n                print(f\"   procesando: {folder}...\")\n                # Recorrer todos los archivos dentro de la carpeta\n                for root, dirs, files in os.walk(folder_path):\n                    for file in files:\n                        file_path = os.path.join(root, file)\n                        # Guardar manteniendo la estructura de carpetas\n                        arcname = os.path.relpath(file_path, \"/kaggle/working\")\n                        zipf.write(file_path, arcname)\n            else:\n                print(f\"âš ï¸ OJO: No encontrÃ© la carpeta '{folder}'. Â¿Seguro que terminÃ³ el entrenamiento?\")\n    \n    if found_any:\n        print(f\"\\nâœ… Â¡Ã‰XITO! Archivo creado: {zip_name}\")\n        print(f\"ğŸ‘‰ Ve al panel derecho 'Output', refresca si es necesario, y descarga el ZIP.\")\n    else:\n        print(\"\\nâŒ No se encontrÃ³ ninguna carpeta de modelos. Revisa los nombres.\")\n\n# Ejecutar\ncompress_models()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T15:44:13.928398Z","iopub.execute_input":"2025-12-15T15:44:13.929373Z","iopub.status.idle":"2025-12-15T15:45:26.801691Z","shell.execute_reply.started":"2025-12-15T15:44:13.929346Z","shell.execute_reply":"2025-12-15T15:45:26.801093Z"}},"outputs":[{"name":"stdout","text":"ğŸ“¦ Iniciando compresiÃ³n de: ['expert_cylinder', 'expert_diamond', 'expert_softbox']...\n   procesando: expert_cylinder...\n   procesando: expert_diamond...\n   procesando: expert_softbox...\n\nâœ… Â¡Ã‰XITO! Archivo creado: ASCII_Architect_V2_Models.zip\nğŸ‘‰ Ve al panel derecho 'Output', refresca si es necesario, y descarga el ZIP.\n","output_type":"stream"}],"execution_count":10}]}